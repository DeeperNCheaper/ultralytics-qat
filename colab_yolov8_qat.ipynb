{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ìƒˆ ì„¹ì…˜"
      ],
      "metadata": {
        "id": "IJXc-xvbP1uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR6MGiKIZw_s",
        "outputId": "f9b4bffc-a2ef-41d6-ba1c-fdb610a3910e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.6 ğŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "Setup complete âœ… (12 CPUs, 83.5 GB RAM, 26.3/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/yolov8-qat-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6G22oMpZ3M4",
        "outputId": "8b887095-5320-4537-ba1a-cb77849a0abf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/yolov8-qat-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Zts2AxaHQK",
        "outputId": "2ea0d197-9c5c-4a10-cb5f-c85565743a08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorrt/nvidia_tensorrt-8.4.3.1-cp310-none-linux_x86_64.whl (340.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m340.9/340.9 MB\u001b[0m \u001b[31m301.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11 (from nvidia-tensorrt)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu11/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-cudnn-cu11 (from nvidia-tensorrt)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu11/nvidia-cudnn-cu11-2022.5.19.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-cublas-cu11 (from nvidia-tensorrt)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu11/nvidia-cublas-cu11-2022.4.8.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-cublas-cu117 (from nvidia-cublas-cu11->nvidia-tensorrt)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu117/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl (333.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m333.1/333.1 MB\u001b[0m \u001b[31m277.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu117 (from nvidia-cuda-runtime-cu11->nvidia-tensorrt)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu117/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m325.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu116 (from nvidia-cudnn-cu11->nvidia-tensorrt)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu116/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl (719.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m719.3/719.3 MB\u001b[0m \u001b[31m187.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (0.42.0)\n",
            "Building wheels for collected packages: nvidia-cublas-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11\n",
            "  Building wheel for nvidia-cublas-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cublas-cu11: filename=nvidia_cublas_cu11-2022.4.8-py3-none-any.whl size=15605 sha256=2e864f811b4b6c00ebe68e5906e941b42dd14aaec7e162afda643022cf1b0885\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5vsf17_l/wheels/8f/ed/66/aa1caefa04698673b59e1799fface640a8f6e840e59b9c27e3\n",
            "  Building wheel for nvidia-cuda-runtime-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cuda-runtime-cu11: filename=nvidia_cuda_runtime_cu11-2022.4.25-py3-none-any.whl size=15677 sha256=acf2ff21de14b3a2f1d78a3e775574f22f6468043159b925af9f47777151eace\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5vsf17_l/wheels/13/1a/0c/e13ce07df95cd19a0671df35ef74895be1f1bf2a62437aa1a9\n",
            "  Building wheel for nvidia-cudnn-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cudnn-cu11: filename=nvidia_cudnn_cu11-2022.5.19-py3-none-any.whl size=15599 sha256=aba956acfc6e7d0bc80323b08f57fbd39b0d74777ede0196684e7e98b1a271ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5vsf17_l/wheels/dd/54/d7/1fe1c468474961cf9e43e85e89fa73b9d41f1608b70e753b03\n",
            "Successfully built nvidia-cublas-cu11 nvidia-cuda-runtime-cu11 nvidia-cudnn-cu11\n",
            "Installing collected packages: nvidia-cudnn-cu116, nvidia-cuda-runtime-cu117, nvidia-cublas-cu117, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-2022.4.8 nvidia-cublas-cu117-11.10.1.25 nvidia-cuda-runtime-cu11-2022.4.25 nvidia-cuda-runtime-cu117-11.7.60 nvidia-cudnn-cu11-2022.5.19 nvidia-cudnn-cu116-8.4.0.27 nvidia-tensorrt-8.4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/ultralytics -b main /content/drive/MyDrive/yolov8-qat-main/ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJvnieFLZ6e7",
        "outputId": "3bbb0516-5c2d-49f8-ec9e-a2a5ec822896"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/drive/MyDrive/yolov8-qat-main/ultralytics' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov8n.pt íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
        "!curl -fsSL -o /content/drive/MyDrive/yolov8-qat-main/ultralytics/yolov8n.pt https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt\n",
        "\n",
        "# í•„ìš”í•œ íŒŒì¼ë“¤ ë³µì‚¬\n",
        "!cp /content/drive/MyDrive/yolov8-qat-main/trainer.py /content/drive/MyDrive/yolov8-qat-main/ultralytics/ultralytics/engine/\n",
        "!cp /content/drive/MyDrive/yolov8-qat-main/validator.py /content/drive/MyDrive/yolov8-qat-main/ultralytics/ultralytics/engine/\n",
        "!cp /content/drive/MyDrive/yolov8-qat-main/default.yaml /content/drive/MyDrive/yolov8-qat-main/ultralytics/ultralytics/cfg/\n",
        "!cp /content/drive/MyDrive/yolov8-qat-main/train.py /content/drive/MyDrive/yolov8-qat-main/ultralytics/\n",
        "\n",
        "# QAT ë””ë ‰í† ë¦¬ ë³µì‚¬\n",
        "!mkdir -p /content/drive/MyDrive/yolov8-qat-main/ultralytics/ultralytics/engine/QAT\n",
        "!cp -r /content/drive/MyDrive/yolov8-qat-main/QAT/* /content/drive/MyDrive/yolov8-qat-main/ultralytics/ultralytics/engine/QAT/\n"
      ],
      "metadata": {
        "id": "BZgSqKm6aGh9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip wheel\n",
        "!pip install nvidia-pyindex\n",
        "!wget https://developer.download.nvidia.com/compute/redist/onnx-graphsurgeon/onnx_graphsurgeon-0.3.27-py2.py3-none-any.whl\n",
        "!pip install ./onnx_graphsurgeon-0.3.27-py2.py3-none-any.whl\n",
        "%rm ./onnx_graphsurgeon-0.3.27-py2.py3-none-any*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KITI8-MeeAm",
        "outputId": "092b1bc9-fca6-4382-ce50-afbdbdd5b711"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.2\n",
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8418 sha256=77c26a6621efed3354837e495e54522472da0b0686d097d172d3bd2625c36e23\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m--2024-01-28 01:01:30--  https://developer.download.nvidia.com/compute/redist/onnx-graphsurgeon/onnx_graphsurgeon-0.3.27-py2.py3-none-any.whl\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42150 (41K) [application/octet-stream]\n",
            "Saving to: â€˜onnx_graphsurgeon-0.3.27-py2.py3-none-any.whlâ€™\n",
            "\n",
            "onnx_graphsurgeon-0 100%[===================>]  41.16K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-01-28 01:01:30 (8.08 MB/s) - â€˜onnx_graphsurgeon-0.3.27-py2.py3-none-any.whlâ€™ saved [42150/42150]\n",
            "\n",
            "Processing ./onnx_graphsurgeon-0.3.27-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx-graphsurgeon==0.3.27) (1.23.5)\n",
            "Collecting onnx (from onnx-graphsurgeon==0.3.27)\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->onnx-graphsurgeon==0.3.27) (3.20.3)\n",
            "Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, onnx-graphsurgeon\n",
            "Successfully installed onnx-1.15.0 onnx-graphsurgeon-0.3.27\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-quantization==2.1.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "accRR2b1ezpw",
        "outputId": "97da9b05-215e-46fe-c8c2-5adcf36e93ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-quantization==2.1.3\n",
            "  Downloading pytorch_quantization-2.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-quantization==2.1.3) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-quantization==2.1.3) (1.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pytorch-quantization==2.1.3) (1.11.4)\n",
            "Collecting sphinx-glpi-theme (from pytorch-quantization==2.1.3)\n",
            "  Downloading sphinx_glpi_theme-0.5-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from pytorch-quantization==2.1.3) (3.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pytorch-quantization==2.1.3) (6.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->pytorch-quantization==2.1.3) (0.2.13)\n",
            "Downloading pytorch_quantization-2.1.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_glpi_theme-0.5-py2.py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m976.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sphinx-glpi-theme, pytorch-quantization\n",
            "Successfully installed pytorch-quantization-2.1.3 sphinx-glpi-theme-0.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/yolov8-qat-main/ultralytics\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "!python3 train.py --data=coco.yaml --model=yolov8x.pt --epochs=0 --batch=16 --device=0 --qat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr1_T0cYgmod",
        "outputId": "f8dc1990-b966-4c3d-913a-5742007019a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/yolov8-qat-main/ultralytics\n",
            "Ultralytics YOLOv8.1.6 ğŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=coco.yaml, epochs=0, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train12, exist_ok=False, pretrained=False, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, qat=True, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=2.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train12\n",
            "\n",
            "Dataset 'coco.yaml' images not found âš ï¸, missing path '/content/datasets/coco/val2017.txt'\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip to '/content/datasets/coco2017labels-segments.zip'...\n",
            "100% 169M/169M [00:00<00:00, 419MB/s]\n",
            "Unzipping /content/datasets/coco2017labels-segments.zip to /content/datasets/coco...: 100% 122232/122232 [00:18<00:00, 6577.30file/s]\n",
            "Downloading http://images.cocodataset.org/zips/train2017.zip to '/content/datasets/coco/images/train2017.zip'...\n",
            "Downloading http://images.cocodataset.org/zips/val2017.zip to '/content/datasets/coco/images/val2017.zip'...\n",
            "Downloading http://images.cocodataset.org/zips/test2017.zip to '/content/datasets/coco/images/test2017.zip'...\n",
            "Dataset download success âœ… (1526.0s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "2024-01-28 01:31:48.217596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 01:31:48.217647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 01:31:48.219290: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1   8795008  ultralytics.nn.modules.head.Detect           [80, [320, 640, 640]]         \n",
            "Model summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs\n",
            "\n",
            "Transferred 595/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train12', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco/labels/train2017... 117266 images, 1021 backgrounds, 0 corrupt: 100% 118287/118287 [02:03<00:00, 955.57it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco/labels/train2017.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:08<00:00, 563.20it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/labels/val2017.cache\n",
            "Plotting labels to runs/detect/train12/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "Add QuantConv to model.0.conv\n",
            "Add QuantConv to model.1.conv\n",
            "Add C2fQuantChunk to model.2\n",
            "Add QuantConv to model.2.cv1.conv\n",
            "Add QuantConv to model.2.cv2.conv\n",
            "Add QuantAdd to model.2.m.0\n",
            "Add QuantConv to model.2.m.0.cv1.conv\n",
            "Add QuantConv to model.2.m.0.cv2.conv\n",
            "Add QuantAdd to model.2.m.1\n",
            "Add QuantConv to model.2.m.1.cv1.conv\n",
            "Add QuantConv to model.2.m.1.cv2.conv\n",
            "Add QuantAdd to model.2.m.2\n",
            "Add QuantConv to model.2.m.2.cv1.conv\n",
            "Add QuantConv to model.2.m.2.cv2.conv\n",
            "Add QuantConv to model.3.conv\n",
            "Add C2fQuantChunk to model.4\n",
            "Add QuantConv to model.4.cv1.conv\n",
            "Add QuantConv to model.4.cv2.conv\n",
            "Add QuantAdd to model.4.m.0\n",
            "Add QuantConv to model.4.m.0.cv1.conv\n",
            "Add QuantConv to model.4.m.0.cv2.conv\n",
            "Add QuantAdd to model.4.m.1\n",
            "Add QuantConv to model.4.m.1.cv1.conv\n",
            "Add QuantConv to model.4.m.1.cv2.conv\n",
            "Add QuantAdd to model.4.m.2\n",
            "Add QuantConv to model.4.m.2.cv1.conv\n",
            "Add QuantConv to model.4.m.2.cv2.conv\n",
            "Add QuantAdd to model.4.m.3\n",
            "Add QuantConv to model.4.m.3.cv1.conv\n",
            "Add QuantConv to model.4.m.3.cv2.conv\n",
            "Add QuantAdd to model.4.m.4\n",
            "Add QuantConv to model.4.m.4.cv1.conv\n",
            "Add QuantConv to model.4.m.4.cv2.conv\n",
            "Add QuantAdd to model.4.m.5\n",
            "Add QuantConv to model.4.m.5.cv1.conv\n",
            "Add QuantConv to model.4.m.5.cv2.conv\n",
            "Add QuantConv to model.5.conv\n",
            "Add C2fQuantChunk to model.6\n",
            "Add QuantConv to model.6.cv1.conv\n",
            "Add QuantConv to model.6.cv2.conv\n",
            "Add QuantAdd to model.6.m.0\n",
            "Add QuantConv to model.6.m.0.cv1.conv\n",
            "Add QuantConv to model.6.m.0.cv2.conv\n",
            "Add QuantAdd to model.6.m.1\n",
            "Add QuantConv to model.6.m.1.cv1.conv\n",
            "Add QuantConv to model.6.m.1.cv2.conv\n",
            "Add QuantAdd to model.6.m.2\n",
            "Add QuantConv to model.6.m.2.cv1.conv\n",
            "Add QuantConv to model.6.m.2.cv2.conv\n",
            "Add QuantAdd to model.6.m.3\n",
            "Add QuantConv to model.6.m.3.cv1.conv\n",
            "Add QuantConv to model.6.m.3.cv2.conv\n",
            "Add QuantAdd to model.6.m.4\n",
            "Add QuantConv to model.6.m.4.cv1.conv\n",
            "Add QuantConv to model.6.m.4.cv2.conv\n",
            "Add QuantAdd to model.6.m.5\n",
            "Add QuantConv to model.6.m.5.cv1.conv\n",
            "Add QuantConv to model.6.m.5.cv2.conv\n",
            "Add QuantConv to model.7.conv\n",
            "Add C2fQuantChunk to model.8\n",
            "Add QuantConv to model.8.cv1.conv\n",
            "Add QuantConv to model.8.cv2.conv\n",
            "Add QuantAdd to model.8.m.0\n",
            "Add QuantConv to model.8.m.0.cv1.conv\n",
            "Add QuantConv to model.8.m.0.cv2.conv\n",
            "Add QuantAdd to model.8.m.1\n",
            "Add QuantConv to model.8.m.1.cv1.conv\n",
            "Add QuantConv to model.8.m.1.cv2.conv\n",
            "Add QuantAdd to model.8.m.2\n",
            "Add QuantConv to model.8.m.2.cv1.conv\n",
            "Add QuantConv to model.8.m.2.cv2.conv\n",
            "Add QuantConv to model.9.cv1.conv\n",
            "Add QuantConv to model.9.cv2.conv\n",
            "Add QuantUpsample to model.10\n",
            "Add QuantConcat to model.11\n",
            "Add C2fQuantChunk to model.12\n",
            "Add QuantConv to model.12.cv1.conv\n",
            "Add QuantConv to model.12.cv2.conv\n",
            "Add QuantConv to model.12.m.0.cv1.conv\n",
            "Add QuantConv to model.12.m.0.cv2.conv\n",
            "Add QuantConv to model.12.m.1.cv1.conv\n",
            "Add QuantConv to model.12.m.1.cv2.conv\n",
            "Add QuantConv to model.12.m.2.cv1.conv\n",
            "Add QuantConv to model.12.m.2.cv2.conv\n",
            "Add QuantUpsample to model.13\n",
            "Add QuantConcat to model.14\n",
            "Add C2fQuantChunk to model.15\n",
            "Add QuantConv to model.15.cv1.conv\n",
            "Add QuantConv to model.15.cv2.conv\n",
            "Add QuantConv to model.15.m.0.cv1.conv\n",
            "Add QuantConv to model.15.m.0.cv2.conv\n",
            "Add QuantConv to model.15.m.1.cv1.conv\n",
            "Add QuantConv to model.15.m.1.cv2.conv\n",
            "Add QuantConv to model.15.m.2.cv1.conv\n",
            "Add QuantConv to model.15.m.2.cv2.conv\n",
            "Add QuantConv to model.16.conv\n",
            "Add QuantConcat to model.17\n",
            "Add C2fQuantChunk to model.18\n",
            "Add QuantConv to model.18.cv1.conv\n",
            "Add QuantConv to model.18.cv2.conv\n",
            "Add QuantConv to model.18.m.0.cv1.conv\n",
            "Add QuantConv to model.18.m.0.cv2.conv\n",
            "Add QuantConv to model.18.m.1.cv1.conv\n",
            "Add QuantConv to model.18.m.1.cv2.conv\n",
            "Add QuantConv to model.18.m.2.cv1.conv\n",
            "Add QuantConv to model.18.m.2.cv2.conv\n",
            "Add QuantConv to model.19.conv\n",
            "Add QuantConcat to model.20\n",
            "Add C2fQuantChunk to model.21\n",
            "Add QuantConv to model.21.cv1.conv\n",
            "Add QuantConv to model.21.cv2.conv\n",
            "Add QuantConv to model.21.m.0.cv1.conv\n",
            "Add QuantConv to model.21.m.0.cv2.conv\n",
            "Add QuantConv to model.21.m.1.cv1.conv\n",
            "Add QuantConv to model.21.m.1.cv2.conv\n",
            "Add QuantConv to model.21.m.2.cv1.conv\n",
            "Add QuantConv to model.21.m.2.cv2.conv\n",
            "Add QuantConv to model.22.cv2.0.0.conv\n",
            "Add QuantConv to model.22.cv2.0.1.conv\n",
            "Add QuantConv to model.22.cv2.0.2\n",
            "Add QuantConv to model.22.cv2.1.0.conv\n",
            "Add QuantConv to model.22.cv2.1.1.conv\n",
            "Add QuantConv to model.22.cv2.1.2\n",
            "Add QuantConv to model.22.cv2.2.0.conv\n",
            "Add QuantConv to model.22.cv2.2.1.conv\n",
            "Add QuantConv to model.22.cv2.2.2\n",
            "Add QuantConv to model.22.cv3.0.0.conv\n",
            "Add QuantConv to model.22.cv3.0.1.conv\n",
            "Add QuantConv to model.22.cv3.0.2\n",
            "Add QuantConv to model.22.cv3.1.0.conv\n",
            "Add QuantConv to model.22.cv3.1.1.conv\n",
            "Add QuantConv to model.22.cv3.1.2\n",
            "Add QuantConv to model.22.cv3.2.0.conv\n",
            "Add QuantConv to model.22.cv3.2.1.conv\n",
            "Add QuantConv to model.22.cv3.2.2\n",
            "Add QuantConv to model.22.dfl.conv\n",
            "Collect stats for calibrating: 100% 1024/1024 [06:29<00:00,  2.63it/s]\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0128 01:40:57.166392 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.166678 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.166794 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.166875 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.166973 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167065 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167156 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167231 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167334 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167410 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167497 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167571 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167653 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167726 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167812 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167885 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.167971 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168057 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168139 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168211 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168296 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168373 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168458 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168530 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168611 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168693 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168771 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168853 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.168926 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169026 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169101 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169187 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169259 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169356 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169429 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169514 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169585 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169664 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169736 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169820 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169893 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.169978 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170061 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170141 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170213 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170298 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170375 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170461 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170533 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170611 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170683 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170767 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170837 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170922 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.170993 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171084 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171156 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171241 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171312 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171404 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171475 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171554 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171627 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171714 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171787 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171874 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.171946 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172036 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172109 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172185 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172267 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172345 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172433 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172504 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172588 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172659 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172750 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172821 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172907 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.172979 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173071 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173143 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173227 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173299 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173389 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173460 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173540 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173610 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173694 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173767 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173850 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.173921 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174014 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174087 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174172 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174242 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174336 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174408 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174487 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174558 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174641 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174712 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174797 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174868 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.174951 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175033 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175119 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175191 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175309 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175389 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175469 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175539 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175614 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175697 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175768 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175855 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.175927 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176028 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176104 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176196 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176268 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176359 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176431 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176510 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176582 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176666 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176736 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176819 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176891 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.176970 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177052 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177139 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177211 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177296 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177371 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177449 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177521 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177596 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177683 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177754 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177838 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.177911 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178010 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178096 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178168 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178251 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178322 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178413 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178484 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178575 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178646 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178731 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178801 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178888 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.178958 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.179053 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.179126 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182057 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182176 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182272 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182354 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182440 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182524 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182606 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182678 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182764 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182837 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.182926 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183011 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183111 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183184 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183271 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183350 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183440 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183511 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183596 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183667 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183754 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183825 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183910 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.183985 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184079 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184162 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184233 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184315 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184392 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184476 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184547 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184631 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184701 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184793 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184865 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.184955 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185054 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185152 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185224 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185310 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185388 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185476 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185547 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185631 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185702 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185782 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185863 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.185935 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186025 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186101 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186186 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186261 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186352 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186423 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186516 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186589 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186674 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186745 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186832 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186904 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.186990 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187075 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187164 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187235 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187322 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187399 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187479 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187573 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187646 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187731 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187803 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187886 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.187958 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.188069 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.188144 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.188230 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.188308 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.207572 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.207765 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.207908 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208032 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208155 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208245 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208354 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208444 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208558 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208649 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208758 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208851 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.208957 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209065 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209175 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209269 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209385 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209473 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209561 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209635 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209722 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209794 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209884 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.209956 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.210076 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.210152 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.210239 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.210311 132190761882752 tensor_quantizer.py:174] Disable MaxCalibrator\n",
            "W0128 01:40:57.210525 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.210607 132190761882752 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
            "W0128 01:40:57.210764 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.210900 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.211018 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.211143 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.211248 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.211366 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.211465 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.211585 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.211686 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.211797 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.211898 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.212012 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212111 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212221 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212320 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.212439 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212537 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.212643 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212737 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212844 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.212945 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.213071 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.213173 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.213276 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.213380 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.213480 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.213585 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.213684 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.213798 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.213898 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.214026 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.214130 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.214249 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.214359 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.214472 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.214570 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.214673 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.214767 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.214874 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.214977 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.215145 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.215339 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.215540 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.215755 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.216059 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.216259 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.216387 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.216487 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.216593 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.216694 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.216804 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.216906 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.217028 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.217140 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.217245 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.217344 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.217452 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.217550 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.217658 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.217755 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.217858 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.217953 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218073 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218173 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.218281 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218383 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.218484 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218578 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218675 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218778 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.218875 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.218987 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.219098 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.219207 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.219303 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.219426 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.219524 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.219633 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.219728 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.219828 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.219921 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.233114 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.233358 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.233495 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.233605 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.233716 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.233814 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.233929 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.234050 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.234166 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.234267 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.234378 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.234474 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.234582 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.234681 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.234791 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.234891 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.235010 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235110 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235218 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235316 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.235432 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235529 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.235631 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235724 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235828 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.235927 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.236089 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.236200 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.236306 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.236406 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.236505 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.236613 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.236710 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.236825 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.236925 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.237048 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.237150 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.237266 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.237372 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.237482 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.237577 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.237678 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.237770 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.237883 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.237981 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.238107 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.238205 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.238306 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.238406 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.238510 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.238610 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.238719 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.238815 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.238917 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.239018 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.239121 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.239231 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.239335 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.239445 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.308411 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.308726 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.308900 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.309054 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.309213 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.309368 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.309534 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.309675 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.309841 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.309973 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.310110 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.310213 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.310339 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.310440 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.310551 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.310648 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.310760 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.310856 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.310979 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311093 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.311197 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311300 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311410 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311504 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311611 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311709 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.311820 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.311920 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.312050 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.312153 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.312263 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.312367 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.312479 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.312576 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.312684 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.312779 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.312890 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.312996 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.313121 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.313215 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([160, 1, 1, 1]).\n",
            "W0128 01:40:57.313317 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.313430 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.313525 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.313628 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.313720 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.313827 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.313925 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.314051 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.314151 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.314267 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.314369 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.314476 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.314569 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.314687 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.314782 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.314890 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.314989 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.315116 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.315212 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.315319 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.315420 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.315520 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.315621 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.315715 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.315816 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.315910 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.316028 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.316128 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.316236 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.316344 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([640, 1, 1, 1]).\n",
            "W0128 01:40:57.316462 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.316559 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.316665 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.316761 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.316873 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.316969 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.317094 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.317189 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.317297 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.317399 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.317503 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.317596 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.317694 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.317811 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.317908 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.318036 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.318142 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.318250 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.318353 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
            "W0128 01:40:57.318463 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.318557 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.318663 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.318758 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.318860 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.318957 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
            "W0128 01:40:57.319078 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.319175 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.319282 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.319382 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.319486 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.319579 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
            "W0128 01:40:57.319717 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.319816 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.319926 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.320037 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.320150 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.320245 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.320359 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.320457 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.320565 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.320660 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.320761 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.320855 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.320961 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.321071 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.321180 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.321272 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([320, 1, 1, 1]).\n",
            "W0128 01:40:57.321380 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.321480 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([80, 1, 1, 1]).\n",
            "W0128 01:40:57.321585 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "W0128 01:40:57.321676 132190761882752 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
            "E0128 01:40:57.330257 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.332520 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.338130 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.340652 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.363697 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.363886 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.375326 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.375483 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.397795 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.397961 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.407065 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.409781 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.444812 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.449518 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.452036 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.454614 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.476799 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.482597 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.485144 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.487198 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.496079 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.496205 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.497755 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.497858 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.505889 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.505995 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.507723 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.507821 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.512767 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.512876 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.514780 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.514885 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.525987 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.526111 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.527585 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.527678 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.535573 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.535671 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.537190 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.537280 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.542033 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.542128 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.543641 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.543732 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.557110 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.557235 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.559591 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.559699 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.572322 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.572438 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.574785 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.574898 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.583218 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.583336 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.590608 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.590721 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.593178 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.593286 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.606993 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.607133 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.609466 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.609579 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.630851 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.630989 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.633388 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.633499 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.664577 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.664718 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.667148 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.667263 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.697565 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.697730 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.700110 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.700239 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.712357 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.712507 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.715116 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.715271 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.723070 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.737786 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.740194 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.740310 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.753201 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.753323 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.755612 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.755725 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.768091 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.768213 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.770527 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.770642 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.778171 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.778301 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.780671 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.780784 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.794115 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.794251 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.796602 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.796719 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.809434 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.809654 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.813132 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.813336 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.834541 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.834795 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.837128 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.837237 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.849610 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.849703 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.851178 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.851262 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.858786 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.858883 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.860332 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.860415 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.864953 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.865065 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.866504 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.866585 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.874512 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.874601 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.876053 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.876134 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.883682 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.883770 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.885236 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.885317 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.889889 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.889974 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.891404 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.891485 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.899574 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.899662 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.901113 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.901194 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.908879 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.908965 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.910408 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.910490 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.916644 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.916745 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.921216 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.921301 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.922748 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.922829 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.931061 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.931148 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.932605 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.932688 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.940721 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.940808 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.942254 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.942336 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.950695 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.950785 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.952261 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.952342 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.960321 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.960408 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.961822 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.961912 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.969542 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.969629 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.971065 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.971144 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.976283 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.976377 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.977834 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.977916 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.985834 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.985920 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.987406 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.987487 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.994930 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.995029 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.996472 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:57.996552 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.001098 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.001182 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.002606 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.002687 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.010656 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.010741 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.012187 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.012267 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.020111 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.020197 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.021625 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.021705 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.026242 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.026334 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.027728 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.027809 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.037329 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.037424 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.038881 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.038964 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.046588 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.046677 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.048109 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.048189 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.052705 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.052792 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.054219 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.054306 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.062232 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.062325 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.063767 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.063848 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.071419 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.071507 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.072955 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.073047 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.077587 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.077670 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.079087 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.079166 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.087158 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.087243 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.088653 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.088734 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.096232 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.096325 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.097749 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.097831 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.102949 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.103047 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.107491 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.107576 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.108983 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.109077 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.118113 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.118200 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.119664 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.119747 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.127763 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.127849 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.129314 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.129394 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.137692 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.137780 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.139240 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.139327 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.147214 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.147309 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.148742 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.148825 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.156318 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.156402 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.157812 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.157892 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.162415 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.162500 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.163926 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.164016 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.171946 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.172042 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.173461 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.173542 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.181112 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.181199 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.182633 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.182716 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.187305 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.187390 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.188816 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.188896 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.196841 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.196928 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.198379 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.198459 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.206037 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.206135 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.207560 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.207639 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.212680 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.212763 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.219816 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.219912 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.221583 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.221673 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.229985 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.230095 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.231546 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.231627 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.243809 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.243899 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.247897 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.247982 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.249433 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.249514 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.253989 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.254099 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.255526 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.255604 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.263577 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.263660 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.265125 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.265203 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.273386 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.273471 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.274877 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.274956 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.282732 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.282815 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.284241 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.284328 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.293394 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.293480 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.294885 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.294965 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.302787 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.302873 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.304282 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.304366 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.313446 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.313531 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.314938 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.315030 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.323965 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.324081 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.325537 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.325617 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.335200 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.335285 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.339344 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.339424 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.343333 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.343414 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.344814 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.344893 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.349402 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.349501 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.351016 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.351095 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.359033 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.359119 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.360532 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.360611 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.368834 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.368919 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.370346 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.370424 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.378383 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.378469 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.379879 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.379961 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.389066 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.389154 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.390577 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.390656 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.398462 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.398550 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.399968 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.400060 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.409166 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.409251 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.410663 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.410743 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.418534 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.418623 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.420041 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.420121 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.429656 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.429744 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.434049 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.434134 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.435546 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.435626 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.443227 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.443319 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.444726 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.444809 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.449262 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.449352 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.450767 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.450847 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.458653 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.458740 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.460156 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.460233 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.468467 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.468555 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.469964 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.470059 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.477970 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.478069 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.479497 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.479577 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.488657 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.488751 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.490176 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.490256 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.498063 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.498148 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.499562 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.499648 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.508730 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.508819 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.510233 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.510318 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.518107 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.518192 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.519632 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.519737 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.530348 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.530436 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.534785 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.534871 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.536315 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.536395 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.543954 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.544053 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.545489 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.545568 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.549973 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.550076 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.551616 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.551697 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.559522 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.559608 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.561025 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.561103 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.569283 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.569375 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.570778 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.570858 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.578659 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.578747 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.580158 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.580237 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.589316 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.589406 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.590825 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.590906 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.598727 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.598821 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.600245 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.600338 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.609793 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.609883 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.611312 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.611396 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.619195 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.619285 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.620704 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.620789 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.630353 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.630452 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.636231 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.636333 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.637796 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.637903 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.646121 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.646213 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.647633 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.647721 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.655344 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.655429 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.656834 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.656915 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.662505 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.662588 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.663990 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.664080 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.671859 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.671946 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.673374 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.673454 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.681052 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.681139 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.682559 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.682641 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.688245 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.688333 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.689735 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.689815 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.697589 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.697677 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.699090 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.699171 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.706743 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.706828 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.708247 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.708335 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.715198 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.715288 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.716711 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.716792 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.725565 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.725655 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.727069 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.727151 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.735286 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.735386 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.736796 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.736877 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.742419 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.742501 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.743915 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.744015 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.751824 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.751907 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.753334 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.753411 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.760958 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.761052 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.762464 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.762542 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.768101 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.768183 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.769600 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.769679 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.777470 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.777555 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.778969 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.779061 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.786707 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.786795 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.788220 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.788306 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.794753 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.794838 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.796260 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:40:58.796344 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.964071 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.964223 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.965919 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.966023 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.974082 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.974171 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.975602 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.975683 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.983811 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.983903 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.985345 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.985424 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.993312 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.993401 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.994828 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:00.994908 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.003331 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.003418 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.004858 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.004938 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.012882 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.012966 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.014404 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.014482 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.021973 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.022068 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.023483 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.023562 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.028151 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.028233 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.029643 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.029722 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.037694 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.037779 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.039218 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.039296 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.046779 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.046868 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.048301 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.048378 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.052891 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.052973 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.054396 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.054474 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.062428 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.062515 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.063954 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.064042 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.071525 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.071609 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.073068 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.073144 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.078217 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.078300 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.082647 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.082729 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.084168 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.084245 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.092383 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.092468 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.093891 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.093969 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.101862 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.101947 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.103376 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.103454 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.111772 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.111862 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.113283 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.113360 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.121237 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.121320 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.122731 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.122810 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.130315 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.130398 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.131793 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.131880 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.136407 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.136489 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.137920 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.138008 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.145852 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.145936 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.147345 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.147424 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.154929 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.155034 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.156459 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.156543 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.161099 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.161181 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.162619 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.162698 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.171303 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.171393 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.172843 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.172924 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.180432 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.180515 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.181914 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.181995 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.186539 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.186622 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.188050 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.188127 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.195932 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.196025 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.197418 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.197498 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.204797 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.204881 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.206300 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.206379 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.210798 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.210882 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.212283 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.212369 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.220066 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.220149 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.221534 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.221614 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.228891 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.228977 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.230385 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.230463 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.236025 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.236109 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.237865 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.237942 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.245910 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.246007 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.247402 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.247481 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.254772 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.254860 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.256250 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.256327 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.261295 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.261377 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.265656 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.265738 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.267144 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.267220 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.275136 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.275218 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.276599 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.276677 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.285242 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.285325 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.286726 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.286806 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.295144 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.295228 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.296746 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.296846 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.304943 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.305062 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.306491 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.306571 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.314436 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.314520 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.315941 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.316041 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.320525 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.320607 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.321987 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.322080 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.329802 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.329894 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.331299 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.331378 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.338653 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.338736 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.340132 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.340209 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.344618 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.344700 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.346128 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.346204 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.353889 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.353974 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.355371 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.355449 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.362717 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.362802 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.364201 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.364279 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.368707 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.368787 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.370179 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.370255 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.378023 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.378109 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.379502 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.379581 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.386890 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.386974 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.388369 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.388447 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.392849 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.392930 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.394321 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.394400 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.402172 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.402254 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.403671 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.403752 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.411199 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.411284 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.412686 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.412766 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.417272 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.417355 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.418739 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.418823 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.426580 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.426662 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.428073 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.428150 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.436533 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.436619 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.438056 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.438133 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.443133 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.443215 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.447552 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.447638 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.449047 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.449124 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.457138 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.457222 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.458657 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.458736 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.466640 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.466725 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.468155 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.468234 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.476416 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.476500 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.477900 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.477979 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.485794 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.485880 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.487292 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.487378 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.494743 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.494826 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.496226 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.496310 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.500714 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.500797 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.502204 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.502285 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.510061 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.510146 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.511556 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.511636 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.519050 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.519135 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.520533 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.520613 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.525086 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.525168 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.526592 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.526673 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.534443 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.534531 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.535950 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.536045 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.543822 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.543907 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.545338 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.545421 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.550329 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.550411 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.554884 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.554968 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.556389 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.556468 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.564214 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.564308 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.565722 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.565803 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.575582 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.575667 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.579522 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.579605 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.581018 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.581097 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.585476 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.585559 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.586962 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.587056 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.594821 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.594907 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.596317 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.596397 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.604452 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.604540 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.605954 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.606048 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.614231 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.614322 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.615733 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.615814 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.624809 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.624896 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.626346 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.626425 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.634187 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.634271 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.635686 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.635768 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.645915 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.646018 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.647435 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.647515 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.655343 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.655432 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.656836 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.656918 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.666496 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.666580 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.670672 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.670756 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.674617 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.674699 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.676108 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.676187 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.680548 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.680631 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.682049 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.682126 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.689949 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.690045 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.691446 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.691525 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.699682 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.699767 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.701174 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.701253 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.709063 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.709146 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.710578 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.710658 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.719690 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.719775 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.721181 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.721260 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.729068 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.729153 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.730562 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.730642 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.739694 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.739780 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.741194 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.741273 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.749078 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.749162 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.750568 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.750647 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.760146 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.760229 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.764538 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.764621 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.766053 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.766130 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.773624 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.773713 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.775125 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.775204 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.779628 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.779713 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.781147 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.781226 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.789109 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.789197 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.790607 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.790689 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.798815 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.798902 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.800326 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.800406 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.808485 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.808571 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.809979 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.810072 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.820283 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.820377 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.821772 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.821853 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.829802 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.829892 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.831349 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.831427 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.841640 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.841729 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.843538 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.843635 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.851852 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.851937 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.853351 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.853430 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.862816 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.862899 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.867175 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.867256 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.868657 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.868737 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.876300 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.876390 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.877766 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.877845 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.882237 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.882324 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.883711 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.883788 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.891627 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.891709 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.893119 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.893196 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.901492 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.901576 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.902975 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.903065 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.910901 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.910984 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.912403 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.912481 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.921579 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.921662 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.923102 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.923179 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.930993 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.931089 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.932472 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.932551 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.941633 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.941717 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.943111 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.943189 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.951061 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.951143 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.952558 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.952636 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.962198 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.962281 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.967187 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.967271 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.968670 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.968749 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.976550 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.976634 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.978041 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.978117 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.985692 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.985774 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.987179 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.987257 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.992781 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.992862 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.994274 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:01.994357 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.002215 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.002297 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.003707 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.003785 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.011400 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.011482 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.012892 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.012970 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.018587 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.018669 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.020092 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.020169 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.027923 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.028018 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.029429 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.029506 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.037124 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.037208 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.038628 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.038707 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.046433 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.046518 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.047928 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.048021 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.055745 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.055827 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.057221 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.057299 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.064788 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.064870 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.066300 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.066384 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.071922 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.072014 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.073435 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.073513 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.081390 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.081478 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.082875 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.082955 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.090497 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.090582 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.091973 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.092063 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.097573 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.097655 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.099068 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.099144 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.106926 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.107020 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.108433 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.108510 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.116088 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.116169 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.117584 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.117661 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.124316 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.124402 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.125835 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:02.125915 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.414595 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.414749 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.414816 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.414859 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.416353 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.416454 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.416507 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.416549 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.422182 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.422274 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.422330 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.422371 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.423678 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.423761 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.423820 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.423861 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.429355 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.429441 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.429493 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.429534 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.430819 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.430899 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.430950 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.430996 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.436700 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.436801 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.436861 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.436906 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.438327 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.438413 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.438468 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.438517 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.444227 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.444318 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.444372 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.444413 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.445683 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.445762 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.445813 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.445853 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.450926 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.451023 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.451077 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.451118 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.452378 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.452457 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.452507 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.452546 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.457402 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.457484 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.458936 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.459031 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.463347 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.463429 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.463479 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.463519 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.464768 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.464847 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.464898 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.464937 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.470061 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.470142 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.470192 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.470231 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.471487 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.471565 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.471615 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.471655 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.476573 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.476656 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.478095 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.478174 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.482424 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.482506 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.482558 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.482599 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.483850 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.483930 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.483981 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.484037 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.489091 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.489172 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.489226 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.489267 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.490520 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.490598 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.490653 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.490693 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.495483 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.495566 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.496952 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.497044 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.501871 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.501954 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.506013 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.506099 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.506151 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.506192 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.507442 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.507518 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.507567 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.507607 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.512767 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.512849 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.512900 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.512941 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.514186 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.514262 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.514317 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.514358 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.519371 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.519452 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.519504 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.519544 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.520832 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.520911 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.520963 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.521014 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.526391 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.526472 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.526523 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.526563 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.527803 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.527881 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.527931 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.527971 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.532923 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.533015 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.533069 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.533109 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.534351 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.534427 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.534477 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.534519 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.539267 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.539353 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.540743 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.540821 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.544983 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.545092 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.545147 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.545188 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.546430 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.546506 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.546556 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.546594 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.551529 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.551610 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.551662 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.551703 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.552937 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.553026 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.553079 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.553120 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.557882 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.557962 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.559360 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.559437 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.563634 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.563717 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.563769 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.563811 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.565066 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.565145 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.565201 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.565248 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.570280 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.570367 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.570418 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.570459 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.571698 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.571776 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.571826 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.571866 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.576619 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.576700 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.578101 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.578178 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.582326 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.582408 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.582460 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.582500 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.583741 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.583818 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.583869 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.583909 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.588910 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.588994 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.589059 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.589101 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.590339 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.590418 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.590469 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.590510 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.595310 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.595392 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.596783 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.596863 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.601125 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.601205 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.601256 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.601297 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.602540 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.602617 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.602667 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.602708 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.607733 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.607817 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.607868 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.607908 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.609179 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.609259 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.609316 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.609358 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.614093 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.614174 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.615560 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.615639 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.619803 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.619889 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.619942 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.619983 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.621276 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.621361 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.621413 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.621454 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.626466 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.626548 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.626600 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.626643 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.627910 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.627990 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.628054 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.628095 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.632894 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.632977 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.634387 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.634465 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.639404 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.639485 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.643523 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.643606 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.643657 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.643700 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.644927 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.645015 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.645084 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.645129 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.650339 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.650422 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.650473 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.650513 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.651746 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.651825 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.651875 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.651916 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.656852 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.656934 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.656985 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.657036 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.658269 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.658353 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.658403 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.658443 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.664392 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.664495 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.664567 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.664624 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.666658 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.666759 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.666839 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.666904 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.672283 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.672373 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.672423 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.672464 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.673704 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.673782 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.673831 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.673871 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.678635 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.678717 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.680153 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.680232 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.684448 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.684530 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.684581 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.684622 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.685881 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.685960 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.686020 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.686061 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.690991 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.691084 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.691135 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.691176 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.692417 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.692493 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.692543 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.692583 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.697436 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.697518 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.698898 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.698977 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.703150 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.703231 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.703287 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.703335 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.704567 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.704644 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.704695 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.704735 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.709716 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.709801 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.709854 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.709894 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.711136 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.711211 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.711261 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.711308 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.716042 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.716125 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.717575 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.717654 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.721874 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.721958 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.722020 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.722062 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.723314 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.723392 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.723440 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.723479 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.728449 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.728532 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.728582 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.728622 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.729861 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.729938 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.729988 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.730040 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.734787 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.734868 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.736275 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.736360 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.740533 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.740616 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.740667 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.740707 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.741941 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.742030 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.742082 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.742123 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.747079 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.747160 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.747210 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.747251 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.748481 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.748558 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.748608 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.748674 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.753376 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.753455 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.754827 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.754907 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.759094 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.759174 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.759225 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.759264 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.760508 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.760586 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.760636 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.760676 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.765618 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.765698 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.765751 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.765790 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.767033 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.767108 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.767157 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.767196 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.771958 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.772051 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.773423 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.773500 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.778396 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.778478 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.782487 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.782570 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.782621 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.782661 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.783892 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.783971 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.784030 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.784071 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.789307 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.789390 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.789442 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.789482 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.790720 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.790797 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.790847 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.790887 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.795829 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.795911 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.795961 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.796010 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.797247 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.797329 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.797381 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.797421 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.802726 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.802809 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.802860 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.802901 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.804146 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.804222 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.804271 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.804317 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.809267 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.809355 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.809408 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.809448 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.810681 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.810760 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.810810 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.810851 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.815575 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.815657 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.817100 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.817177 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.821397 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.821478 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.821530 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.821571 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.822806 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.822885 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.822935 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.822975 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.827968 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.828060 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.828113 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.828154 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.829388 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.829465 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.829515 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.829555 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.834261 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.834349 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.835732 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.835810 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.839965 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.840057 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.840109 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.840150 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.841403 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.841480 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.841529 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.841570 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.846516 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.846598 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.846650 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.846691 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.847916 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.847993 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.848057 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.848097 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.852789 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.852870 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.854253 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.854336 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.859127 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.859209 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.863500 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.863595 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.863677 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.863741 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.865453 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.865532 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.865583 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.865634 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.871092 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.871174 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.871225 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.871266 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.872501 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.872579 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.872630 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.872670 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.879935 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.880029 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.883852 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.883935 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.885355 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.885434 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.889551 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.889630 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.889681 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.889721 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.890955 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.891055 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.891108 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.891148 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.896115 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.896199 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.896250 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.896291 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.897532 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.897609 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.897659 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.897699 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.903017 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.903100 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.903151 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.903192 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.904432 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.904511 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.904560 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.904601 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.909569 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.909652 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.909703 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.909744 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.910970 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.911059 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.911110 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.911149 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.917342 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.917425 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.917475 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.917515 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.918757 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.918834 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.918885 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.918924 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.923920 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.924015 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.924069 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.924109 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.925374 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.925451 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.925500 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.925540 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.931720 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.931804 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.931855 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.931896 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.933138 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.933213 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.933262 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.933308 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.938248 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.938336 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.938389 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.938429 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.939671 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.939749 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.939800 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.939840 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.946692 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.946774 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.950791 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.950874 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.954702 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.954785 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.956184 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.956262 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.960383 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.960466 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.960517 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.960558 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.961800 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.961879 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.961929 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.961969 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.967154 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.967265 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.967360 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.967424 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.968934 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.969034 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.969109 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.969153 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.974528 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.974617 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.974673 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.974716 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.975962 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.976059 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.976115 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.976164 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.981115 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.981199 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.981254 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.981297 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.982537 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.982618 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.982671 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.982714 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.989102 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.989185 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.989236 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.989276 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.990517 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.990593 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.990643 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.990681 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.995749 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.995833 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.995885 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.995925 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.997166 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.997243 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:08.997292 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:08.997343 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.003561 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.003646 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.003697 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.003738 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.004968 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.005074 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.005128 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.005172 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.010126 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.010206 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.010257 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.010297 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.011550 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.011627 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.011676 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.011715 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.018563 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.018646 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.022779 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.022862 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.022914 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.022954 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.024193 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.024271 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.024327 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.024371 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.029356 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.029438 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.030841 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.030921 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.035103 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.035188 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.035240 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.035281 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.036525 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.036601 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.036652 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.036693 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.041646 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.041728 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.041780 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.041829 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.043076 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.043152 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.043201 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.043242 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.048597 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.048680 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.048731 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.048771 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.049996 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.050084 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.050134 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.050173 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.055157 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.055239 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.055290 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.055339 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.056582 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.056661 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.056711 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.056750 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.062911 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.062995 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.063059 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.063099 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.064340 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.064416 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.064465 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.064505 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.070661 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.070746 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.070798 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.070838 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.072325 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.072404 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.072454 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.072497 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.078790 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.078875 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.078927 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.078973 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.080215 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.080295 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.080352 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.080393 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.085381 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.085463 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.085512 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.085553 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.086790 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.086868 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.086918 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.086958 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.093742 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.093828 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.097899 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.097983 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.098046 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.098087 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.099333 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.099410 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.099459 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.099498 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.104434 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.104515 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.105934 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.106025 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.110119 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.110200 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.110250 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.110291 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.111529 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.111606 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.111660 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.111703 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.116667 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.116749 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.116801 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.116842 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.118092 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.118169 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.118218 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.118258 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.123617 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.123700 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.123752 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.123793 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.125031 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.125122 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.125174 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.125215 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.130159 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.130241 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.130292 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.130341 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.131570 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.131647 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.131696 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.131736 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.137937 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.138033 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.138088 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.138129 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.139369 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.139444 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.139493 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.139533 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.144490 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.144572 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.144623 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.144664 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.145915 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.145993 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.146055 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.146094 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.152258 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.152346 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.152399 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.152440 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.153677 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.153755 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.153805 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.153845 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.158807 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.158890 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.158942 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.158983 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.160218 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.160295 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.160354 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.160394 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.167270 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.167362 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.172761 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.172913 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.172990 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.173058 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.174379 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.174460 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.174511 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.174551 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.179561 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.179644 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.179696 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.179736 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.180968 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.181060 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.181111 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.181151 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.186129 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.186215 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.187645 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.187730 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.192956 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.193051 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.193103 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.193144 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.194392 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.194467 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.194518 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.194558 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.199548 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.199629 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.199681 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.199722 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.200953 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.201042 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.201094 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.201134 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.206104 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.206190 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.207585 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.207664 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.212896 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.212978 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.213041 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.213083 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.214330 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.214407 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.214458 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.214498 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.219472 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.219553 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.219604 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.219645 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.220861 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.220938 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.220988 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.221040 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.226073 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.226159 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.227550 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.227628 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.234086 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.234168 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.234218 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.234258 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.235503 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.235580 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.235633 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.235674 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.240634 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.240718 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.240770 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.240810 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.242050 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.242126 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.242176 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.242217 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.247216 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.247298 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.248695 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.248774 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.254013 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.254094 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.254146 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.254186 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.255422 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.255500 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.255549 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.255590 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.260514 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.260597 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.260648 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.260688 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.261929 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.262017 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.262069 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.262110 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.267057 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.267138 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.268509 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.268588 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.275788 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.275872 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.275924 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.275964 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.277222 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.277307 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.277359 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.277399 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.282384 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.282466 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.282516 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.282557 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.283788 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.283865 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.283915 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.283955 132190761882752 tensor_quantizer.py:135] step_size is undefined under dynamic amax mode!\n",
            "E0128 01:41:09.288982 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.289079 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.290490 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.290567 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.297033 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.297114 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.298501 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "E0128 01:41:09.298580 132190761882752 tensor_quantizer.py:121] Fake quantize mode doesn't use scale explicitly!\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mWARNING âš ï¸ TensorBoard graph visualization failure NYI: Named tensors are not supported with the tracer\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train12\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20      35.9G      1.115      1.488      1.259        210        640:  43% 3162/7393 [32:42<41:55,  1.68it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "!python3 train.py --data=coco.yaml --model=yolov8l.pt --epochs=0 --batch=16 --device=0 --qat\n"
      ],
      "metadata": {
        "id": "7r0dAB00jeiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "!python3 train.py --data=coco.yaml --model=yolov8m.pt --epochs=0 --batch=16 --device=0 --qat\n"
      ],
      "metadata": {
        "id": "nLslKhS-jfwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "!python3 train.py --data=coco.yaml --model=yolov8s.pt --epochs=0 --batch=16 --device=0 --qat\n"
      ],
      "metadata": {
        "id": "UYXaZx-wjhXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "!python3 train.py --data=coco.yaml --model=yolov8n.pt --epochs=0 --batch=16 --device=0 --qat"
      ],
      "metadata": {
        "id": "y_FpewLpjiK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}